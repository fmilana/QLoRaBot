{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft==0.11.1 bitsandbytes accelerate \"trl<0.9.0\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/model/fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT HUGGINGFACE_TOKEN BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# https://huggingface.co/settings/tokens\n",
    "login(token=\"*huggingface_token*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/finetune_model.py\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "chosen_user = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "def format_example(example):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': f'You are an assistant that mimics {chosen_user} in a WhatsApp group chat with his friends that switch between English and Italian. Respond naturally in {chosen_user}\\'s style to the conversation below. The message should take the context into account so that it is coherent and flows naturally with the conversation. When {chosen_user} is mentioned in any of the messages in the conversation, you should pay more attention to that message when replying.'}\n",
    "    ]\n",
    "    \n",
    "    for entry in example[\"context\"]:\n",
    "        messages.append({'role': 'user', 'content': f\"[{entry['speaker']}]: {entry['message']}\"})\n",
    "    \n",
    "    messages.append({'role': 'assistant', 'content': example['target_response']})\n",
    "    \n",
    "    return {'text': tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "\n",
    "def find_all_linear_names(model, bits):\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit if bits == 4 else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n",
    "    )\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Fine-tune a language model with QLoRA')\n",
    "    parser.add_argument('--model-name', type=str, default='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                        help='Model name to load (default: \"meta-llama/Meta-Llama-3-8B-Instruct\")')\n",
    "    parser.add_argument('--user', type=str, default=None,\n",
    "                        help='User name to load specific train/val files (e.g., \"Paolo\")')\n",
    "    parser.add_argument('--min-msg-length', type=int, default=0,\n",
    "                        help='Minimum number of words in messages (default: 0)')\n",
    "    parser.add_argument('--epochs', type=int, default=2,\n",
    "                        help='Number of training epochs (default: 2)')\n",
    "    parser.add_argument('--train-batch-size', type=int, default=8,\n",
    "                        help='Per device batch size for training (default: 4)')\n",
    "    parser.add_argument('--eval-batch-size', type=int, default=4,\n",
    "                        help='Per device batch size for evaluation (default: 4)')\n",
    "    parser.add_argument('--drive-path', type=str, default='/content/drive/MyDrive/qlora',\n",
    "                        help='Path in Google Drive for data and model output')\n",
    "    parser.add_argument('--patience', type=int, default=3,\n",
    "                        help='Early stopping patience. Number of evaluation calls with no improvement after which training will stop (default: 3)')\n",
    "    parser.add_argument('--early-stopping', action='store_true',\n",
    "                        help='Enable early stopping based on validation loss')\n",
    "    # New arguments for configurable parameters\n",
    "    parser.add_argument('--lora-rank', type=int, default=4,\n",
    "                        help='Rank for LoRA adaptation (default: 4)')\n",
    "    parser.add_argument('--lora-alpha', type=float, default=None,\n",
    "                        help='Alpha for LoRA adaptation (default: 2 * rank)')\n",
    "    parser.add_argument('--lora-dropout', type=float, default=0.05,\n",
    "                        help='Dropout for LoRA adaptation (default: 0.05)')\n",
    "    parser.add_argument('--eval-steps', type=float, default=500,\n",
    "                        help='Number of steps between evaluations (default: 500, can also use fractions like 0.2)')\n",
    "    parser.add_argument('--save-steps', type=float, default=500,\n",
    "                        help='Number of steps between saving checkpoints (default: 500, can also use fractions like 0.2)')\n",
    "    parser.add_argument('--save-total-limit', type=int, default=2,\n",
    "                        help='Maximum number of checkpoints to keep (default: 2)')\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-5,\n",
    "                        help='Learning rate for fine-tuning (default: 1e-5)')\n",
    "    parser.add_argument('--logging-steps', type=int, default=50,\n",
    "                        help='Number of steps between logging updates (default: 50)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    chosen_user = args.user\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    rank = args.lora_rank\n",
    "    lora_alpha = args.lora_alpha if args.lora_alpha is not None else rank * 2\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name\n",
    "    )\n",
    "\n",
    "    PAD_TOKEN = '<|pad|>'\n",
    "\n",
    "    special_tokens = ['[Federico]:', '[Paolo]:', '[Riccardo Santini]:', '[Guglielmone]:']\n",
    "    tokenizer.add_special_tokens({'pad_token': PAD_TOKEN, 'additional_special_tokens': special_tokens})\n",
    "\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    target_modules = [\n",
    "        'gate_proj',\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'o_proj',\n",
    "        'down_proj',\n",
    "        'up_proj',\n",
    "        'lm_head'\n",
    "    ]\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=args.lora_dropout,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM'\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_dataset_path = f'{args.drive_path}/data/processed/train_conversations_{args.user.lower()}.json'\n",
    "    val_dataset_path = f'{args.drive_path}/data/processed/val_conversations_{args.user.lower()}.json'\n",
    "\n",
    "    with open(f'{args.drive_path}/data/processed/train_conversations_{args.user.lower()}.json', 'r', encoding='utf-8') as f:\n",
    "        raw_training_data = json.load(f)\n",
    "\n",
    "    with open(f'{args.drive_path}/data/processed/val_conversations_{args.user.lower()}.json', 'r', encoding='utf-8') as f:\n",
    "        raw_validation_data = json.load(f)\n",
    "\n",
    "    print(f'Formatting training dataset...')\n",
    "    train_examples = [\n",
    "        conversation for conversation in raw_training_data\n",
    "        if len(conversation['target_response'].split()) >= args.min_msg_length\n",
    "    ]\n",
    "    train_dataset = Dataset.from_list(train_examples)\n",
    "    train_dataset = train_dataset.map(format_example, remove_columns=train_dataset.column_names)\n",
    "\n",
    "    print(f'Formatting validation dataset...')\n",
    "    val_examples = [\n",
    "        conversation for conversation in raw_validation_data\n",
    "        if len(conversation['target_response'].split()) >= args.min_msg_length\n",
    "    ]\n",
    "    val_dataset = Dataset.from_list(val_examples)\n",
    "    val_dataset = val_dataset.map(format_example, remove_columns=val_dataset.column_names)\n",
    "\n",
    "    print(f'Training dataset size: {len(train_dataset)}')\n",
    "    print(f'Validation dataset size: {len(val_dataset)}')\n",
    "\n",
    "    response_template = '<|end_header_id|>'\n",
    "\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        response_template=response_template,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    print(f\"LoRA Configuration:\")\n",
    "    print(f\"  - Rank: {rank}\")\n",
    "    print(f\"  - Alpha: {lora_alpha}\")\n",
    "    print(f\"  - Dropout: {args.lora_dropout}\")\n",
    "    print(f\"Training Parameters:\")\n",
    "    print(f\"  - Learning rate: {args.learning_rate}\")\n",
    "    print(f\"  - Evaluation steps: {args.eval_steps}\")\n",
    "    print(f\"  - Save steps: {args.save_steps}\")\n",
    "    print(f\"  - Save total limit: {args.save_total_limit}\")\n",
    "\n",
    "    training_args = SFTConfig(\n",
    "        dataset_text_field='text',  # this is the default column name\n",
    "        max_seq_length=512,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=args.epochs,\n",
    "        warmup_steps=4,\n",
    "        learning_rate=args.learning_rate,\n",
    "        fp16=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_safetensors=True,\n",
    "        save_steps=args.save_steps,\n",
    "        save_strategy='steps',\n",
    "        save_total_limit=args.save_total_limit,\n",
    "        logging_steps=args.logging_steps,\n",
    "        output_dir=f'{args.drive_path}/model/training',\n",
    "        optim='paged_adamw_8bit',\n",
    "        run_name=f'finetune-{args.user}-{args.model_name.split(\"/\")[-1]}',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    callbacks = []\n",
    "    if args.early_stopping:\n",
    "        print(f\"Early stopping enabled with patience {args.patience}\")\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=args.patience))\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the best model\n",
    "    print(\"Saving the best model...\")\n",
    "    model.save_pretrained(f'{args.drive_path}/model/fine-tuned')\n",
    "    tokenizer.save_pretrained(f'{args.drive_path}/model/fine-tuned')\n",
    "\n",
    "    print('Model fine-tuning complete.')\n",
    "    print(f'Model saved to {args.drive_path}/model/fine-tuned')\n",
    "    print(f'Tokenizer saved to {args.drive_path}/model/fine-tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload dataset files to Google Drive:\n",
    "/content/drive/MyDrive/finetuning/data/processed/train_conversations.json\n",
    "/content/drive/MyDrive/finetuning/data/processed/val_conversations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/finetune_model.py \\\n",
    "  --model-name \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "  --user \"paolo\" \\\n",
    "  --min-msg-length 0 \\\n",
    "  --learning-rate 7e-5 \\\n",
    "  --epochs 5 \\\n",
    "  --early-stopping \\\n",
    "  --patience 3 \\\n",
    "  --lora-rank 4 \\\n",
    "  --lora-alpha 8 \\\n",
    "  --lora-dropout 0.05 \\\n",
    "  --eval-steps 100 \\\n",
    "  --save-steps 100 \\\n",
    "  --save-total-limit 3\n",
    "\n",
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save adapter weights from model/fine-tuned to local machine after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/test_model.py\n",
    "import torch\n",
    "import gc\n",
    "import argparse\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "chosen_user = None\n",
    "\n",
    "def load_fine_tuned_model(model_path, base_model_name):\n",
    "    # Free up CUDA memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Configure quantization for inference\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load the base model with quantization\n",
    "    print(f\"Loading base model: {base_model_name}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(f\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Add the same special tokens that were used during training\n",
    "    special_tokens = ['[Federico]:', '[Paolo]:', '[Riccardo Santini]:', '[Guglielmone]:']\n",
    "    pad_token = '<|pad|>'\n",
    "    tokenizer.add_special_tokens({'pad_token': pad_token, 'additional_special_tokens': special_tokens})\n",
    "    \n",
    "    # Set padding side to match training\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "    # Resize model embeddings to match tokenizer size\n",
    "    base_model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "    \n",
    "    # Load the fine-tuned LoRA adapter\n",
    "    print(f\"Loading adapter from: {model_path}\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=100, temperature=0.7, top_p=0.9, repetition_penalty=1.2):\n",
    "    # Create input tokens with attention mask\n",
    "    inputs = tokenizer(\n",
    "        prompt, \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096,  # Match max_seq_length from training\n",
    "        return_attention_mask=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate with the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the newly generated tokens\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the response\n",
    "    cleaned_response = generated_text.strip()\n",
    "    # Remove \"assistant\" prefix if present\n",
    "    if cleaned_response.lower().startswith(\"assistant\"):\n",
    "        cleaned_response = cleaned_response[len(\"assistant\"):].strip()\n",
    "    \n",
    "    return cleaned_response\n",
    "\n",
    "\n",
    "def parse_conversation_input(user_input):\n",
    "    messages = []\n",
    "    parts = user_input.split(\"|\")\n",
    "    \n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if \":\" in part:\n",
    "            speaker, message = part.split(\":\", 1)\n",
    "            messages.append({\n",
    "                \"speaker\": speaker.strip(),\n",
    "                \"message\": message.strip()\n",
    "            })\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def format_conversation(messages, chosen_user):\n",
    "    # Format using the chat template to match training\n",
    "    system_message = {\n",
    "        'role': 'system', \n",
    "        'content': f'You are an assistant that mimics {chosen_user} in a WhatsApp group chat with his friends that switch between English and Italian. Respond naturally in {chosen_user}\\'s style to the conversation below. The message should take the context into account so that it is coherent and flows naturally with the conversation. When {chosen_user} is mentioned in any of the messages in the conversation, you should pay more attention to that message when replying.'\n",
    "    }\n",
    "    \n",
    "    user_messages = [\n",
    "        {'role': 'user', 'content': f\"[{message['speaker']}]: {message['message']}\"} \n",
    "        for message in messages\n",
    "    ]\n",
    "    \n",
    "    # Apply the chat template without the assistant's response\n",
    "    formatted_conversation = tokenizer.apply_chat_template(\n",
    "        [system_message] + user_messages,\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    return formatted_conversation\n",
    "\n",
    "\n",
    "def interactive_mode(model, tokenizer, chosen_user):\n",
    "    print(\"\\n===== Interactive Mode =====\")\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    print(\"Format your input as: 'Guglielmone: Message1 | Paolo: Message2 | ...'\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter conversation: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        messages = parse_conversation_input(user_input)\n",
    "        conversation = format_conversation(messages, chosen_user)\n",
    "        \n",
    "        print(\"\\nFormatted prompt:\")\n",
    "        print(conversation)\n",
    "        print(\"\\nGenerating response...\")\n",
    "        \n",
    "        completion = generate_response(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            conversation\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n[{chosen_user}]: {completion}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Test a fine-tuned model\")\n",
    "    parser.add_argument('--user', type=str, default=\"Paolo\",\n",
    "                        help='User name to be used in the prompt.')\n",
    "    parser.add_argument(\"--model-path\", type=str, default=\"/content/drive/MyDrive/qlora/model/fine-tuned/\", \n",
    "                        help=\"Path to the fine-tuned model\")\n",
    "    parser.add_argument(\"--base-model\", type=str, default=\"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "                        help=\"Name of the base model\")\n",
    "    parser.add_argument(\"--prompt\", type=str, \n",
    "                        help=\"Single prompt to test (if not provided, interactive mode will start)\")\n",
    "    parser.add_argument(\"--max-length\", type=int, default=100, \n",
    "                        help=\"Maximum length of generated text\")\n",
    "    parser.add_argument(\"--temperature\", type=float, default=0.7, \n",
    "                        help=\"Sampling temperature\")\n",
    "    parser.add_argument(\"--top-p\", type=float, default=0.9, \n",
    "                        help=\"Top-p sampling parameter\")\n",
    "    parser.add_argument(\"--repetition-penalty\", type=float, default=1.2, \n",
    "                        help=\"Repetition penalty\")\n",
    "    parser.add_argument(\"--drive-path\", type=str, default=\"/content/drive/MyDrive/qlora\",\n",
    "                        help=\"Path in Google Drive for data and model\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    chosen_user = args.user\n",
    "    \n",
    "    # Update model path if drive path is provided\n",
    "    if args.drive_path and not args.model_path.startswith(\"/content/drive\"):\n",
    "        args.model_path = os.path.join(args.drive_path, \"model/fine-tuned\")\n",
    "    \n",
    "    # Check if model path exists\n",
    "    if not os.path.exists(args.model_path):\n",
    "        print(f\"Warning: Model path {args.model_path} does not exist.\")\n",
    "        print(f\"Checking Google Drive path...\")\n",
    "        if os.path.exists(\"/content/drive\"):\n",
    "            potential_paths = [\n",
    "                \"/content/drive/MyDrive/qlora/model/fine-tuned\",\n",
    "                \"/content/drive/MyDrive/qlora/model/fine-tuned/adapter_model.bin\"\n",
    "            ]\n",
    "            for path in potential_paths:\n",
    "                if os.path.exists(path):\n",
    "                    args.model_path = os.path.dirname(path)\n",
    "                    print(f\"Found model at: {args.model_path}\")\n",
    "                    break\n",
    "    \n",
    "    # Load the model\n",
    "    model, tokenizer = load_fine_tuned_model(args.model_path, args.base_model)\n",
    "    \n",
    "    if args.prompt:\n",
    "        # Single prompt mode\n",
    "        print(f\"\\nPrompt: {args.prompt}\")\n",
    "\n",
    "        messages = parse_conversation_input(args.prompt)\n",
    "        formatted_prompt = format_conversation(messages, chosen_user)\n",
    "\n",
    "        print(f\"\\nFormatted prompt: {formatted_prompt}\")\n",
    "        \n",
    "        completion = generate_response(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            formatted_prompt,\n",
    "            max_length=args.max_length,\n",
    "            temperature=args.temperature,\n",
    "            top_p=args.top_p,\n",
    "            repetition_penalty=args.repetition_penalty\n",
    "        )\n",
    "        print(f\"[{chosen_user}]: {completion}\")\n",
    "    else:\n",
    "        # Interactive mode\n",
    "        interactive_mode(model, tokenizer, chosen_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(login(token=\"*huggingface_token*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/test_model.py \\\n",
    "  --user Paolo"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
