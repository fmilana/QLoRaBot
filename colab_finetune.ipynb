{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RezSxlIi-S0J",
        "outputId": "f12d5b19-a76e-4760-c791-48f173cbd8ae"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers peft bitsandbytes accelerate trl datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp79auMe--ym",
        "outputId": "b3091585-2d7e-42c6-d54b-1adebcceee63"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2--xaP7U_FRt"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/model/fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yeuFpJjBTj6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Get your token from https://huggingface.co/settings/tokens\n",
        "login(token=\"*huggingface_token*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaCVzne9_H9t",
        "outputId": "da71bb05-1253-43dc-91a5-09c2cf756cc5"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/finetune_model.py\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import argparse\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    logging\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel\n",
        ")\n",
        "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "# Set PyTorch memory allocation configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Set max sequence length\n",
        "MAX_LENGTH = 2048\n",
        "\n",
        "\n",
        "def format_prompts_func(example_or_batch):\n",
        "    def format_single(example):\n",
        "        conversation = \"\"\n",
        "        for entry in example[\"context\"]:\n",
        "            conversation += f\"[{entry['speaker']}]: {entry['message']}\\n\"\n",
        "        conversation += f\"[RESPONSE]: {example['target_response']}\"\n",
        "        return conversation\n",
        "\n",
        "    # Detect batch\n",
        "    if isinstance(example_or_batch[\"context\"][0], dict):\n",
        "        # Single example\n",
        "        return [format_single(example_or_batch)]\n",
        "    else:\n",
        "        # Batch\n",
        "        return [format_single({\"context\": ctx, \"target_response\": resp})\n",
        "                for ctx, resp in zip(example_or_batch[\"context\"], example_or_batch[\"target_response\"])]\n",
        "\n",
        "\n",
        "def filter_by_length(examples, tokenizer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Filter examples that would exceed the maximum length when tokenized.\n",
        "    Returns a boolean mask of which examples to keep.\n",
        "    \"\"\"\n",
        "    formatted_examples = format_prompts_func(examples)\n",
        "    tokenized_lengths = [len(tokenizer.encode(example)) for example in formatted_examples]\n",
        "    return [length <= max_length for length in tokenized_lengths]\n",
        "\n",
        "\n",
        "def load_raw_dataset(file_path, tokenizer, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Load raw dataset from JSON file and filter by tokenized length.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    # Create a temporary dataset to use with the filter function\n",
        "    temp_dataset = Dataset.from_list(raw_data)\n",
        "\n",
        "    # Apply length filtering\n",
        "    print(f\"Filtering examples longer than {max_length} tokens...\")\n",
        "    filtered_dataset = temp_dataset.filter(\n",
        "        lambda examples: filter_by_length(examples, tokenizer, max_length),\n",
        "        batched=True,\n",
        "        batch_size=100  # Process in batches for efficiency\n",
        "    )\n",
        "\n",
        "    print(f\"Original dataset size: {len(temp_dataset)}\")\n",
        "    print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
        "\n",
        "    return filtered_dataset\n",
        "\n",
        "\n",
        "def train_with_qlora(train_dataset, val_dataset, model_name, output_dir, epochs=2, batch_size=1):\n",
        "    \"\"\"\n",
        "    Fine-tune a model using QLoRA with memory optimizations.\n",
        "    \"\"\"\n",
        "    # Free up CUDA memory before starting\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,  # Double quantization to save more memory\n",
        "    )\n",
        "\n",
        "    # Load the model with quantization config\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        offload_folder=\"offload\",  # Enable offloading to CPU/disk\n",
        "        offload_state_dict=True,  # Offload state dict when not in use\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Add speaker tags as special tokens\n",
        "    speaker_tokens = ['[Federico]:', '[Paolo]:', '[Riccardo Santini]:', '[Guglielmone]:']\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': speaker_tokens})\n",
        "\n",
        "    # Resize model embeddings to accommodate new tokens\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Determine target modules based on model architecture\n",
        "    if \"deepseek\" in model_name.lower():\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "    elif \"llama\" in model_name.lower() or \"mistral\" in model_name.lower() or \"mixtral\" in model_name.lower():\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "    elif \"bloom\" in model_name.lower():\n",
        "        target_modules = [\"query_key_value\"]\n",
        "    elif \"falcon\" in model_name.lower():\n",
        "        target_modules = [\"query_key_value\"]\n",
        "    else:\n",
        "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules\n",
        "    )\n",
        "\n",
        "    # Get PEFT model\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print trainable parameters info\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = SFTConfig(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        gradient_accumulation_steps=8,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=10,\n",
        "        learning_rate=1e-4,\n",
        "        save_strategy=\"steps\",\n",
        "        eval_strategy=\"steps\",\n",
        "        save_steps=500,  # Save less frequently than evaluation\n",
        "        eval_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=2,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.05,\n",
        "        bf16=True,\n",
        "        tf32=True,\n",
        "        max_grad_norm=0.3,\n",
        "        logging_dir=\"./logs\",\n",
        "        fp16=False,\n",
        "        report_to=\"none\",\n",
        "        packing=False,\n",
        "        max_seq_length=MAX_LENGTH  # Set maximum sequence length\n",
        "    )\n",
        "\n",
        "    # Set up the response template for completion-only training\n",
        "    response_template = \"[RESPONSE]:\"\n",
        "\n",
        "    # Create a data collator that masks loss for tokens before the response template\n",
        "    data_collator = DataCollatorForCompletionOnlyLM(\n",
        "        response_template=response_template,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Create SFTTrainer with completion-only data collator and formatting_func\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        args=training_args,\n",
        "        formatting_func=format_prompts_func,  # Use the formatting function\n",
        "        data_collator=data_collator,         # Use completion-only data collator\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Calculate perplexity explicitly on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "    eval_loss = eval_results[\"eval_loss\"]\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss)).item()\n",
        "\n",
        "    print(f\"Final validation loss: {eval_loss}\")\n",
        "    print(f\"Final validation perplexity: {perplexity}\")\n",
        "\n",
        "    # Save the model\n",
        "    trainer.save_model(output_dir)\n",
        "\n",
        "    return model, tokenizer, perplexity\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up argument parser\n",
        "    parser = argparse.ArgumentParser(description='Fine-tune a language model with QLoRA')\n",
        "    parser.add_argument('--model-name', type=str, default=\"meta-llama/Meta-Llama-3-8B\",\n",
        "                        help='Base model to use (default: meta-llama/Meta-Llama-3-8B)')\n",
        "    parser.add_argument('--output-name', type=str, required=True,\n",
        "                        help='Custom name for the output model directory')\n",
        "    parser.add_argument('--user', type=str, default=None,\n",
        "                        help='User name to load specific train/val files (e.g., \"paolo_v1\")')\n",
        "    parser.add_argument('--epochs', type=int, default=2,\n",
        "                        help='Number of training epochs (default: 2)')\n",
        "    parser.add_argument('--batch-size', type=int, default=1,\n",
        "                        help='Batch size for training (default: 1)')\n",
        "    parser.add_argument('--drive-path', type=str, default=\"/content/drive/MyDrive/finetuning\",\n",
        "                        help='Path in Google Drive for data and model output')\n",
        "\n",
        "    # Parse arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Free up CUDA memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Define model name and output directory with custom name\n",
        "    model_name = args.model_name\n",
        "    drive_path = args.drive_path\n",
        "    output_dir = f\"{drive_path}/model/fine-tuned/{args.output_name}\"\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load tokenizer first for length filtering\n",
        "    print(f\"Loading tokenizer from {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Add speaker tags as special tokens\n",
        "    speaker_tokens = ['[Federico]:', '[Paolo]:', '[Riccardo Santini]:', '[Guglielmone]:']\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': speaker_tokens})\n",
        "\n",
        "    # Determine file paths based on user parameter\n",
        "    if args.user:\n",
        "        train_dataset_path = f\"{drive_path}/data/processed/train_conversations_{args.user}.json\"\n",
        "        val_dataset_path = f\"{drive_path}/data/processed/val_conversations_{args.user}.json\"\n",
        "    else:\n",
        "        train_dataset_path = f\"{drive_path}/data/processed/train_conversations.json\"\n",
        "        val_dataset_path = f\"{drive_path}/data/processed/val_conversations.json\"\n",
        "\n",
        "    print(f\"Loading training dataset from {train_dataset_path}\")\n",
        "    train_dataset = load_raw_dataset(train_dataset_path, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    print(f\"Loading validation dataset from {val_dataset_path}\")\n",
        "    val_dataset = load_raw_dataset(val_dataset_path, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "    # Train the model with QLoRA and completion-only loss\n",
        "    model, tokenizer, val_perplexity = train_with_qlora(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        model_name,\n",
        "        output_dir,\n",
        "        epochs=args.epochs,\n",
        "        batch_size=args.batch_size\n",
        "    )\n",
        "\n",
        "    print(f\"Model fine-tuning complete. Model saved to {output_dir}\")\n",
        "    print(f\"Final validation perplexity: {val_perplexity}\")\n",
        "    print(\"To test the model, use test_model.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9o0mLwY_UcT",
        "outputId": "27e1aad9-b59a-4fe3-8229-12935b126ba7"
      },
      "outputs": [],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUurolhc_fqL"
      },
      "source": [
        "Upload dataset files to Google Drive:\n",
        "/content/drive/MyDrive/finetuning/data/processed/train_conversations.json\n",
        "/content/drive/MyDrive/finetuning/data/processed/val_conversations.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuPvuVAA_oEr",
        "outputId": "33e88a16-e238-4162-ff2e-fcc7c1d3dd48"
      },
      "outputs": [],
      "source": [
        "!python /content/finetune_model.py \\\n",
        "  --model-name \"meta-llama/Meta-Llama-3-8B\" \\\n",
        "  --output-name \"paolo_v1\" \\\n",
        "  --user \"paolo\" \\\n",
        "  --epochs 2 \\\n",
        "  --batch-size 8 \\\n",
        "  --drive-path \"/content/drive/MyDrive/qlora\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM0sGEiaAQJ5"
      },
      "source": [
        "Save adapter weights from model/ to local machine after training"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
