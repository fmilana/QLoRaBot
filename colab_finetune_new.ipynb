{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft==0.11.1 bitsandbytes accelerate trl<0.9.0 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/model/fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT HUGGINGFACE_TOKEN BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Get your token from https://huggingface.co/settings/tokens\n",
    "login(token=\"*huggingface_token*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/finetune_model.py\n",
    "import os\n",
    "import argparse\n",
    "import warnings\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from datasets import Dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "chosen_user = None\n",
    "\n",
    "\n",
    "def load_raw_dataset(file_path, min_words):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    examples = [\n",
    "        conversation for conversation in raw_data \n",
    "        if len(conversation['target_response'].split()) >= min_words\n",
    "    ]\n",
    "\n",
    "    filtered_dataset = Dataset.from_list(examples)\n",
    "\n",
    "    print(f\"Original dataset size: {len(raw_data)}\")\n",
    "    print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def format_prompts_func(example_or_batch):\n",
    "    def format_single(example):\n",
    "        conversation = f\"<|system|>\\nYou are {chosen_user} in a WhatsApp group chat with his friends that switch between English and Italian. Respond naturally in {chosen_user}'s style to the conversation below. The message should take the context into account so that it is coherent and flows naturally with the conversation. When {chosen_user} is mentioned in any of the messages in the conversation, you should pay more attention to that message when replying.</|system|>\\n\\n\"\n",
    "\n",
    "        for entry in example[\"context\"]:\n",
    "            conversation += f\"[{entry['speaker']}]: {entry['message']}\\n\"\n",
    "\n",
    "        conversation += f\"[RESPONSE]: {example['target_response']}\"\n",
    "\n",
    "        return conversation\n",
    "\n",
    "    if isinstance(example_or_batch[\"context\"][0], dict):\n",
    "        return [format_single(example_or_batch)]\n",
    "    else:\n",
    "        return [format_single({\"context\": ctx, \"target_response\": resp}) \n",
    "                for ctx, resp in zip(example_or_batch[\"context\"], example_or_batch[\"target_response\"])]\n",
    "\n",
    "\n",
    "def find_all_linear_names(model, bits):\n",
    "    cls = (\n",
    "        bnb.nn.Linear4bit if bits == 4 else (bnb.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n",
    "    )\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    \n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Fine-tune a language model with QLoRA')\n",
    "    parser.add_argument('--user', type=str, default=None,\n",
    "                        help='User name to load specific train/val files (e.g., \"Paolo\")')\n",
    "    parser.add_argument('--min-msg-length', type=int, default=0,\n",
    "                        help='Minimum number of words in messages (default: 0)')\n",
    "    parser.add_argument('--epochs', type=int, default=2,\n",
    "                        help='Number of training epochs (default: 2)')\n",
    "    parser.add_argument('--batch-size', type=int, default=1,\n",
    "                        help='Batch size for training (default: 1)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    chosen_user = args.user\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    model_name_or_path = 'meta-llama/Meta-Llama-3-8B'\n",
    "\n",
    "    rank = 16\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    special_tokens = ['[Federico]:', '[Paolo]:', '[Riccardo Santini]:', '[Guglielmone]:', '<|system|>', '</|system|>']\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    target_modules = find_all_linear_names(model, 8)\n",
    "    target_modules.append('lm_head')\n",
    "    target_modules = [\n",
    "        'gate_proj',\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'o_proj',\n",
    "        'down_proj',\n",
    "        'up_proj',\n",
    "        'gate_proj',\n",
    "        'lm_head'\n",
    "    ]\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=rank,\n",
    "        lora_alpha=rank * 2,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM'\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    train_dataset_path = f'{drive_path}/data/processed/train_conversations_{args.user.lower()}.json'\n",
    "    val_dataset_path = f'{drive_path}/data/processed/val_conversations_{args.user.lower()}.json'\n",
    "        \n",
    "    print(f'Loading training dataset from {train_dataset_path}')\n",
    "    train_dataset = load_raw_dataset(train_dataset_path, args.min_msg_length)\n",
    "\n",
    "    print(f'Loading validation dataset from {val_dataset_path}')\n",
    "    val_dataset = load_raw_dataset(val_dataset_path, args.min_msg_length)\n",
    "\n",
    "    print(f'Training dataset size: {len(train_dataset)}')\n",
    "    print(f'Validation dataset size: {len(val_dataset)}')\n",
    "\n",
    "    response_template = \"[RESPONSE]:\"\n",
    "\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        response_template=response_template,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=transformers.TrainingArguments(\n",
    "            auto_find_batch_size=True,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=args.epochs,\n",
    "            warmup_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            save_safetensors=True,\n",
    "            save_steps=500,\n",
    "            save_strategy='steps',\n",
    "            logging_steps=100,\n",
    "            output_dir='model/training',\n",
    "            optim='paged_adamw_8bit',\n",
    "        ),\n",
    "        formatting_func=format_prompts_func,  # Use the formatting function\n",
    "        data_collator=data_collator,          # Use completion-only data collator\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained('model/fine-tuned')\n",
    "    tokenizer.save_pretrained('model/fine-tuned')\n",
    "\n",
    "    print('Model fine-tuning complete.')\n",
    "    print('Model saved to model/fine-tuned')\n",
    "    print('Tokenizer saved to model/fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload dataset files to Google Drive:\n",
    "/content/drive/MyDrive/finetuning/data/processed/train_conversations.json\n",
    "/content/drive/MyDrive/finetuning/data/processed/val_conversations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /content/finetune_model.py \\\n",
    "  --model-name \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "  --output-name \"paolo_v1\" \\\n",
    "  --user \"paolo\" \\\n",
    "  --min-msg-length 10 \\\n",
    "  --epochs 5 \\\n",
    "  --batch-size 8 \\\n",
    "  --drive-path \"/content/drive/MyDrive/qlora\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save adapter weights from model/ to local machine after training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
